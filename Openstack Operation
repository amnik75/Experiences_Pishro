Cloud-based applications typically request more discrete hardware (horizontal scaling) as opposed to traditional applications, which require larger hardware to 
scale (vertical scaling).
OpenStack is designed to be horizontally scalable. Rather than switching to larger servers, you procure more servers and simply install identically configured 
services.
Hardware does not have to be consistent, but it should at least have the same type of CPU to support instance  live migration but for cold migration it shouldn't
be identical.

Examples of burn-in tests include running a CPU or disk benchmark for several days.

For capacity planning this link is very good. https://docs.openstack.org/operations-guide/ops-capacity-planning-scaling.html

openstack --debug server list # use --debug to show api calls
openstack compute service list --long # how to get an overview of your cloud, its shape, size, and current state.
openstack catalog list
nova diagnostics <serverID>

The admin is global, not per project, so granting a user the admin role in any project gives the user administrative rights across the whole cloud.

ps aux | grep nova-
ps aux | grep glance-
ps aux | grep keystone
ps aux | grep cinder

# If the commands work as expected, you can be confident that those services are in working condition
. openrc
openstack image list
openstack server list
openstack project list

ip route get <ip> # for more information :)

# for Cloud Controller failure this link is good
https://docs.openstack.org/operations-guide/ops-maintenance-controller.html

# for Compute Node Failures and Maintenance this link is good
https://docs.openstack.org/operations-guide/ops-maintenance-compute.html

# istances xml files are in /etc/libvirt/qemu/instance-xxxxxxxx.xml
openstack server reboot --hard <server> # it regerate the xml file and  then reboot it

# data of the instances are in /var/lib/nova/instances

# priority for Handling a Complete Failure use this link
https://docs.openstack.org/operations-guide/ops-maintenance-complete.html

# to mount the disk of openstack instance we should do this.
modprob nbd
sudo qemu-nbd -c /dev/nbd0 rbd:nova/dcd23225-cf06-466f-91da-f866a9cd8791_disk -f raw
mount /dev/nbd0p1 /mnt
umount /mnt
sudo qemu-nbd -d /dev/nbd0

OpenStack does not configure the databases out of the ordinary. Basic administration includes performance tweaking, high availability, backup, recovery, and 
repairing. For more information, see a standard MySQL administration guide.
Also check RabbitMQ message queues that are growing without being consumed which will indicate which OpenStack services are affected. Restart the affected 
OpenStack services.

# to-do items list:
https://docs.openstack.org/operations-guide/ops-maintenance-hdmwy.html#semiannually

Unfortunately, sometimes the error is not apparent from the log files. In this case, switch tactics and use a different command; maybe run the service directly 
on the command line.

# for slowness check this link
https://docs.openstack.org/operations-guide/ops-maintenance-slow.html
Sometimes messages queued for services stay on the queues and are not consumed. This can be due to dead or stagnant services and can be commonly cleared up by 
either restarting the AMQP-related services or the OpenStack service in question.

# to uninstall openstack use this link
https://docs.openstack.org/operations-guide/ops-uninstall.html

# for checking logs of horizon use 	/var/log/apache2/
# for checking logs of dnsmasq use /var/log/syslog
# for checking logs of instances use /var/log/libvirt/qemu/
# for checking logs of instances console use /var/lib/nova/instances/instance-<instance id>/console.log
#for checking more logs of keystone use /var/log/apache2 beside /var/log/keystone

The first step in finding the source of an error is typically to search for a CRITICAL, or ERROR message in the log starting at the bottom of the log file.

When an instance fails to behave properly, you will often have to trace activity associated with that instance across the log files of various nova-* services 
and across both the cloud controller and compute nodes.
The typical way is to trace the UUID associated with an instance across the service logs.
If no ERROR or CRITICAL messages appear, the most recent log entry that reports this may provide a hint about what has gone wrong.

# show the usage of the projects
openstack usage list







