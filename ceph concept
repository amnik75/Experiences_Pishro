 #### Architecture https://docs.ceph.com/en/latest/architecture/
 Ceph OSD Daemons handle read, write, and replication operations on storage drives. With the older Filestore back end, each RADOS object was stored as a separate 
 file on a conventional filesystem (usually XFS). With the new and default BlueStore back end, objects are stored in a monolithic database-like fashion.
 Ceph OSD Daemons store data as objects in a flat namespace (e.g., no hierarchy of directories). An object has an identifier, binary data, and metadata consisting 
 of a set of name/value pairs.
 Ceph OSD Daemons create object replicas on other Ceph Nodes to ensure data safety and high availability. Ceph also uses a cluster of monitors to ensure 
 high availability. To eliminate centralization, Ceph uses an algorithm called CRUSH.
 
 Cluster Map: Monitor Map,OSD Map,PG Map,CRUSH Map,MDS Map
 Each map maintains an iterative history of its operating state changes. Ceph Monitors maintain a master copy of the cluster map including the cluster members, 
 state, changes, and the overall health of the Ceph Storage Cluster.
 
 Before Ceph Clients can read or write data, they must contact a Ceph Monitor to obtain the most recent copy of the cluster map. A Ceph Storage Cluster can operate 
 with a single monitor; however, this introduces a single point of failure.
 
 For added reliability and fault tolerance, Ceph supports a cluster of monitors. In a cluster of monitors, latency and other faults can cause one or more monitors 
 to fall behind the current state of the cluster. For this reason, Ceph must have agreement among various monitor instances regarding the state of the cluster. 
 Ceph always uses a majority of monitors (e.g., 1, 2:3, 3:5, 4:6, etc.)
 
 Cephx uses shared secret keys for authentication, meaning both the client and the monitor cluster have a copy of the client’s secret key.
 
 A key scalability feature of Ceph is to avoid a centralized interface to the Ceph object store, which means that Ceph clients must be able to interact with OSDs 
 directly.
 Cephx uses shared secret keys for authentication, meaning both the client and the monitor cluster have a copy of the client’s secret key.
 A user/actor invokes a Ceph client to contact a monitor. each monitor can authenticate users and distribute keys, so there is no single point of failure or 
 bottleneck when using cephx.
 The monitor returns an authentication data structure similar to a Kerberos ticket that contains a session key for use in obtaining Ceph services. This session key 
 is itself encrypted with the user’s permanent secret key, so that only the user can request services from the Ceph Monitor(s). The client then uses the session 
 key to request its desired services from the monitor, and the monitor provides the client with a ticket that will authenticate the client to the OSDs that actually 
 handle data. Ceph Monitors and OSDs share a secret, so the client can use the ticket provided by the monitor with any OSD or metadata server in the cluster.
 
 cephx tickets expire, so an attacker cannot use an expired ticket or session key obtained surreptitiously.
 
 To use cephx, an administrator must set up users first. In the following diagram, the client.admin user invokes ceph auth get-or-create-key from the 
 command line to generate a username and secret key. Ceph’s auth subsystem generates the username and key, stores a copy with the monitor(s) and transmits 
 the user’s secret back to the client.admin user. This means that the client and the monitor share a secret key.
 
 To authenticate with the monitor, the client passes in the user name to the monitor, and the monitor generates a session key and encrypts it with the secret key 
 associated to the user name. Then, the monitor transmits the encrypted ticket back to the client. The client then decrypts the payload with the shared secret key 
 to retrieve the session key. The session key identifies the user for the current session. The client then requests a ticket on behalf of the user signed by the 
 session key. The monitor generates a ticket, encrypts it with the user’s secret key and transmits it back to the client. The client decrypts the ticket and uses 
 it to sign requests to OSDs and metadata servers throughout the cluster.  
 
 The cephx protocol authenticates ongoing communications between the client machine and the Ceph servers. Each message sent between a client and server, 
 subsequent to the initial authentication, is signed using a ticket that the monitors, OSDs and metadata servers can verify with their shared secret.
 
 The protection offered by this authentication is between the Ceph client and the Ceph server hosts. The authentication is not extended beyond the Ceph client. 
 If the user accesses the Ceph client from a remote host, Ceph authentication is not applied to the connection between the user’s host and the client host.
 
 Ceph provides three types of clients: Ceph Block Device, Ceph File System, and Ceph Object Storage. A Ceph Client converts its data from the representation 
 format it provides to its users (a block device image, RESTful objects, CephFS filesystem directories) into objects for storage in the Ceph Storage Cluster.
 
 The objects Ceph stores in the Ceph Storage Cluster are not striped. Ceph Object Storage, Ceph Block Device, and the Ceph File System stripe their data over 
 multiple Ceph Storage Cluster objects. Ceph Clients that write directly to the Ceph Storage Cluster via librados must perform the striping (and parallel I/O) 
 for themselves to obtain these benefits.
 
 Once the Ceph Client has striped data to stripe units and mapped the stripe units to objects, Ceph’s CRUSH algorithm maps the objects to placement groups, and 
 the placement groups to Ceph OSD Daemons before the objects are stored as files on a storage drive.
 
 Ceph’s Object Storage uses the term object to describe the data it stores. S3 and Swift objects are not the same as the objects that Ceph writes to 
 the Ceph Storage Cluster. Ceph Object Storage objects are mapped to Ceph Storage Cluster objects. The S3 and Swift objects do not necessarily correspond in a 
 1:1 manner with an object stored in the storage cluster. It is possible for an S3 or Swift object to map to multiple Ceph objects.
 
 #### Ceph Storage Cluster https://docs.ceph.com/en/latest/rados/
 Generally speaking, each OSD is backed by a single storage device, like a traditional hard disk (HDD) or solid state disk (SSD). OSDs can also be backed by a 
 combination of devices, like a HDD for most data and an SSD (or partition of an SSD) for some metadata.
 Ceph Monitor daemons manage critical cluster state like cluster membership and authentication informatio
 
 # odd backends
 There are two ways that OSDs can manage the data they store. Starting with the Luminous 12.2.z release, the new default (and recommended) backend is BlueStore. 
 Prior to Luminous, the default (and only option) was Filestore.
 
 All Ceph configuration options have a unique name consisting of words formed with lower-case characters and connected with underscore (_) characters.
 
Each Ceph daemon, process, and library will pull its configuration from several sources, listed below. Sources later in the list will override those earlier in 
the list when both are present.
the compiled-in default value
the monitor cluster’s centralized configuration database
a configuration file stored on the local host
environment variables
command line arguments
runtime overrides set by an administrator

Ceph options that are stored in the monitor configuration database or in local configuration files are grouped into sections to indicate which daemons or clients 
they apply to. global,mon,mgr,osd,mds,client are the sections.

 You may deploy Ceph with a single monitor, but if the instance fails, the lack of other monitors may interrupt data service availability.
 
 It is possible to run a Ceph Storage Cluster with two networks: a public (client, front-side) network and a cluster (private, replication, back-side) network. 
 However, this approach complicates network configuration (both hardware and software) and does not usually have a significant impact on overall performance. 
 For this reason, we recommend that for resilience and capacity dual-NIC systems either active/active bond these interfaces or implemebnt a layer 3 multipath 
 strategy with eg. FRR
 
 If you declare a cluster network, OSDs will route heartbeat, object replication and recovery traffic over the cluster network. This may improve performance 
 compared to using a single network.We prefer that the cluster network is NOT reachable from the public network or the Internet for added security.
 
 The messenger v2 protocol, or msgr2, is the second major revision on Ceph’s on-wire protocol.
 # ceph mon dump
0: [v2:10.0.0.10:3300/0,v1:10.0.0.10:6789/0] mon.foo
1: [v2:10.0.0.11:3300/0,v1:10.0.0.11:6789/0] mon.bar
2: [v2:10.0.0.12:3300/0,v1:10.0.0.12:6789/0] mon.baz
The bracketed list or vector of addresses means that the same daemon can be reached on multiple ports (and protocols). Any client or other daemon connecting to 
that daemon will use the v2 protocol (listed first) if possible; otherwise it will back to the legacy v1 protocol. Legacy clients will only see the v1 addresses 
and will continue to connect as they did before, with the v1 protocol.

The cephx protocol is enabled by default. Cryptographic authentication has some computational costs, though they should generally be quite low. If the network 
environment connecting your client and server hosts is very safe and you cannot afford authentication, you can turn it off. This is not generally recommended.

Ceph Monitors maintain a “master copy” of the Cluster Map, which means a Ceph Client can determine the location of all Ceph Monitors, Ceph OSD Daemons, and 
Ceph Metadata Servers just by connecting to one Ceph Monitor and retrieving a current cluster map.
With a current copy of the cluster map and the CRUSH algorithm, a Ceph Client can compute the location for any object.
 
The primary role of the Ceph Monitor is to maintain a master copy of the cluster map. Ceph Monitors also provide authentication and logging services.
When there is a significant change in the state of the cluster–e.g., a Ceph OSD Daemon goes down, a placement group falls into a degraded state, etc.–the 
cluster map gets updated to reflect the current state of the cluster.Additionally, the Ceph Monitor also maintains a history of the prior states of the cluster. 
The monitor map, OSD map, placement group map and metadata server map each maintain a history of their map versions. We call each version an “epoch.”

Ceph Clients and other Ceph daemons use the Ceph configuration file to discover monitors, monitors discover each other using the monitor map (monmap), not the 
Ceph configuration file.

Each Ceph Storage Cluster has a unique identifier (fsid). If specified, it usually appears under the [global] section of the configuration file. Deployment tools 
usually generate the fsid and store it in the monitor map, so the value may not appear in a configuration file. The fsid makes it possible to run daemons for 
multiple clusters on the same hardware.

When a Ceph Storage Cluster gets close to its maximum capacity (see``mon_osd_full ratio``), Ceph prevents you from writing to or reading from OSDs as a safety 
easure to prevent data loss. Therefore, letting a production Ceph Storage Cluster approach its full ratio is not a good practice, because it sacrifices 
high availability

# read https://docs.ceph.com/en/latest/rados/configuration/mon-config-ref/?#storage-capacity for capacity planning
if some OSDs are nearfull, but others have plenty of capacity, you may have an inaccurate CRUSH weight set for the nearfull OSDs.

Ceph monitors know about the cluster by requiring reports from each OSD, and by receiving reports from OSDs about the status of their neighboring OSDs.

When you run a production cluster with multiple monitors (recommended), each monitor checks to see if a neighboring monitor has a more recent version of 
the cluster map.Periodically, one monitor in the cluster may fall behind the other monitors to the point where it must leave the quorum, synchronize to retrieve 
the most current information about the cluster, and then rejoin the quorum.

Ceph daemons pass critical messages to each other, which must be processed before daemons reach a timeout threshold. If the clocks in Ceph monitors are not 
synchronized, it can lead to a number of anomalies. For example:
Daemons ignoring received messages (e.g., timestamps outdated)
Timeouts triggered too soon/late when a message wasn’t received in time.

LOOKING UP MONITORS THROUGH DNS. This way daemons and clients do not require a mon host configuration directive in their ceph.conf configuration file.This way 
daemons and clients do not require a mon host configuration directive in their ceph.conf configuration file.

Ceph ensures data integrity by scrubbing placement groups. Ceph scrubbing is analogous to fsck on the object storage layer. For each placement group, 
Ceph generates a catalog of all objects and compares each primary object and its replicas to ensure that no objects are missing or mismatched. 
Light scrubbing (daily) checks the object size and attributes. Deep scrubbing (weekly) reads the data and uses checksums to ensure data integrity.

Scrubbing is important for maintaining data integrity, but it can reduce performance. 

ceph use mclock for QOS.

When you add or remove Ceph OSD Daemons to a cluster, CRUSH will rebalance the cluster by moving placement groups to or from Ceph OSDs to restore balanced 
utilization. The process of migrating placement groups and the objects they contain can reduce the cluster’s operational performance considerably. 
To maintain operational performance, Ceph performs this migration with ‘backfilling’, which allows Ceph to set backfill operations to a lower priority than 
requests to read or write data.

If a Ceph OSD Daemon crashes and comes back online, usually it will be out of sync with other Ceph OSD Daemons containing more recent versions of objects in the 
placement groups. When this happens, the Ceph OSD Daemon goes into recovery mode and seeks to get the latest copy of the data and bring its map back up to date. 
Depending upon how long the Ceph OSD Daemon was down, the OSD’s objects and placement groups may be significantly out of date. Also, if a failure domain went 
down (e.g., a rack), more than one Ceph OSD Daemon may come back online at the same time. This can make the recovery process time consuming and resource intensive.

Filestore OSDs use a journal for two reasons: speed and consistency. Note that since Luminous, the BlueStore OSD back end has been preferred and default. 
This information is provided for pre-existing OSDs and for rare situations where Filestore is preferred for new deployments.

When you create pools and set the number of placement groups (PGs) for each, Ceph uses default values when you don’t specifically override the defaults. 
We recommend overriding some of the defaults. Specifically, we recommend setting a pool’s replica size and overriding the default number of placement groups.

MON_DOWN: One or more monitor daemons is currently down. The cluster requires a majority (more than 1/2) of the monitors in order to function. When one or more monitors 
are down, clients may have a harder time forming their initial connection to the cluster as they may need to try more addresses before they reach an 
operating monitor.

The cluster should normally have at least one running manager (ceph-mgr) daemon. If no manager daemon is running, 
the cluster’s ability to monitor itself will be compromised, and parts of the management API will become unavailable (for example, the dashboard will not work, 
and most CLI commands that report metrics or runtime state will block). However, the cluster will still be able to perform all IO operations and recover from 
failures.
The down manager daemon should generally be restarted as soon as possible to ensure that the cluster can be monitored (e.g., so that the ceph -s information is 
up to date, and/or metrics can be scraped by Prometheus).

The utilization thresholds for nearfull, backfillfull, full, and/or failsafe_full are not ascending. In particular, we expect 
nearfull < backfillfull, backfillfull < full, and full < failsafe_full.

POOL_FULL: One or more pools has reached its quota and is no longer allowing writes.

# PG_AVAILABILITY
Data availability is reduced, meaning that the cluster is unable to service potential read or write requests for some data in the cluster. Specifically, 
one or more PGs is in a state that does not allow IO requests to be serviced. Problematic PG states include peering, stale, incomplete, and the lack of active 
(if those conditions do not clear quickly).

# PG_DEGRADED
Data redundancy is reduced for some data, meaning the cluster does not have the desired number of replicas for all data (for replicated pools) or 
erasure code fragments (for erasure coded pools).

# OSD_BACKFILLFULL
One or more OSDs has exceeded the backfillfull threshold, which will prevent data from being allowed to rebalance to this device. This is an early warning that 
rebalancing may not be able to complete and that the cluster is approaching full.

The number of PGs in use in the cluster is above the configurable threshold of mon_max_pg_per_osd PGs per OSD. If this threshold is exceed the cluster 
wll not allow new pools to be created, pool pg_num to be increased, or pool replication to be increased (any of which would lead to more PGs in the cluster). 
A large number of PGs can lead to higher memory utilization for OSD daemons, slower peering after cluster state changes 
(like OSD restarts, additions, or removals), and higher load on the Manager and Monitor daemons.
The simplest way to mitigate the problem is to increase the number of OSDs in the cluster by adding more hardware. Note that the OSD count used for the 
purposes of this health check is the number of “in” OSDs, so marking “out” OSDs “in” (if there are any) can also help:
ceph osd in <osd id(s)>

# OBJECT_MISPLACED
One or more objects in the cluster is not stored on the node the cluster would like it to be stored on. This is an indication that data migration due to some 
recent cluster change has not yet completed.
Misplaced data is not a dangerous condition in and of itself; data consistency is never at risk, and old copies of objects are never removed until the desired 
number of new copies (in the desired locations) are present.

High availability and high reliability require a fault-tolerant approach to managing hardware and software issues. Ceph has no single point-of-failure, and can 
service requests for data in a “degraded” mode. Ceph’s data placement introduces a layer of indirection to ensure that data doesn’t bind directly to particular 
OSD addresses. This means that tracking down system faults requires finding the placement group and the underlying OSDs at root of the problem.

# Tip A fault in one part of the cluster may prevent you from accessing a particular object, but that doesn’t mean that you cannot access other objects. When you 
run into a fault, don’t panic. Just follow the steps for monitoring your OSDs and placement groups. Then, begin troubleshooting.

