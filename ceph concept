 #### Architecture https://docs.ceph.com/en/latest/architecture/
 Ceph OSD Daemons handle read, write, and replication operations on storage drives. With the older Filestore back end, each RADOS object was stored as a separate 
 file on a conventional filesystem (usually XFS). With the new and default BlueStore back end, objects are stored in a monolithic database-like fashion.
 Ceph OSD Daemons store data as objects in a flat namespace (e.g., no hierarchy of directories). An object has an identifier, binary data, and metadata consisting 
 of a set of name/value pairs.
 Ceph OSD Daemons create object replicas on other Ceph Nodes to ensure data safety and high availability. Ceph also uses a cluster of monitors to ensure 
 high availability. To eliminate centralization, Ceph uses an algorithm called CRUSH.
 
 Cluster Map: Monitor Map,OSD Map,PG Map,CRUSH Map,MDS Map
 Each map maintains an iterative history of its operating state changes. Ceph Monitors maintain a master copy of the cluster map including the cluster members, 
 state, changes, and the overall health of the Ceph Storage Cluster.
 
 Before Ceph Clients can read or write data, they must contact a Ceph Monitor to obtain the most recent copy of the cluster map. A Ceph Storage Cluster can operate 
 with a single monitor; however, this introduces a single point of failure.
 
 For added reliability and fault tolerance, Ceph supports a cluster of monitors. In a cluster of monitors, latency and other faults can cause one or more monitors 
 to fall behind the current state of the cluster. For this reason, Ceph must have agreement among various monitor instances regarding the state of the cluster. 
 Ceph always uses a majority of monitors (e.g., 1, 2:3, 3:5, 4:6, etc.)
 
 Cephx uses shared secret keys for authentication, meaning both the client and the monitor cluster have a copy of the client’s secret key.
 
 A key scalability feature of Ceph is to avoid a centralized interface to the Ceph object store, which means that Ceph clients must be able to interact with OSDs 
 directly.
 Cephx uses shared secret keys for authentication, meaning both the client and the monitor cluster have a copy of the client’s secret key.
 A user/actor invokes a Ceph client to contact a monitor. each monitor can authenticate users and distribute keys, so there is no single point of failure or 
 bottleneck when using cephx.
 The monitor returns an authentication data structure similar to a Kerberos ticket that contains a session key for use in obtaining Ceph services. This session key 
 is itself encrypted with the user’s permanent secret key, so that only the user can request services from the Ceph Monitor(s). The client then uses the session 
 key to request its desired services from the monitor, and the monitor provides the client with a ticket that will authenticate the client to the OSDs that actually 
 handle data. Ceph Monitors and OSDs share a secret, so the client can use the ticket provided by the monitor with any OSD or metadata server in the cluster.
 
 cephx tickets expire, so an attacker cannot use an expired ticket or session key obtained surreptitiously.
 
 To use cephx, an administrator must set up users first. In the following diagram, the client.admin user invokes ceph auth get-or-create-key from the 
 command line to generate a username and secret key. Ceph’s auth subsystem generates the username and key, stores a copy with the monitor(s) and transmits 
 the user’s secret back to the client.admin user. This means that the client and the monitor share a secret key.
 
 To authenticate with the monitor, the client passes in the user name to the monitor, and the monitor generates a session key and encrypts it with the secret key 
 associated to the user name. Then, the monitor transmits the encrypted ticket back to the client. The client then decrypts the payload with the shared secret key 
 to retrieve the session key. The session key identifies the user for the current session. The client then requests a ticket on behalf of the user signed by the 
 session key. The monitor generates a ticket, encrypts it with the user’s secret key and transmits it back to the client. The client decrypts the ticket and uses 
 it to sign requests to OSDs and metadata servers throughout the cluster.  
 
 The cephx protocol authenticates ongoing communications between the client machine and the Ceph servers. Each message sent between a client and server, 
 subsequent to the initial authentication, is signed using a ticket that the monitors, OSDs and metadata servers can verify with their shared secret.
 
 The protection offered by this authentication is between the Ceph client and the Ceph server hosts. The authentication is not extended beyond the Ceph client. 
 If the user accesses the Ceph client from a remote host, Ceph authentication is not applied to the connection between the user’s host and the client host.
 
 Ceph provides three types of clients: Ceph Block Device, Ceph File System, and Ceph Object Storage. A Ceph Client converts its data from the representation 
 format it provides to its users (a block device image, RESTful objects, CephFS filesystem directories) into objects for storage in the Ceph Storage Cluster.
 
 The objects Ceph stores in the Ceph Storage Cluster are not striped. Ceph Object Storage, Ceph Block Device, and the Ceph File System stripe their data over 
 multiple Ceph Storage Cluster objects. Ceph Clients that write directly to the Ceph Storage Cluster via librados must perform the striping (and parallel I/O) 
 for themselves to obtain these benefits.
 
 Once the Ceph Client has striped data to stripe units and mapped the stripe units to objects, Ceph’s CRUSH algorithm maps the objects to placement groups, and 
 the placement groups to Ceph OSD Daemons before the objects are stored as files on a storage drive.
 
 Ceph’s Object Storage uses the term object to describe the data it stores. S3 and Swift objects are not the same as the objects that Ceph writes to 
 the Ceph Storage Cluster. Ceph Object Storage objects are mapped to Ceph Storage Cluster objects. The S3 and Swift objects do not necessarily correspond in a 
 1:1 manner with an object stored in the storage cluster. It is possible for an S3 or Swift object to map to multiple Ceph objects.
 
 #### Ceph Storage Cluster https://docs.ceph.com/en/latest/rados/
 Generally speaking, each OSD is backed by a single storage device, like a traditional hard disk (HDD) or solid state disk (SSD). OSDs can also be backed by a 
 combination of devices, like a HDD for most data and an SSD (or partition of an SSD) for some metadata.
 Ceph Monitor daemons manage critical cluster state like cluster membership and authentication informatio
 
 # odd backends
 There are two ways that OSDs can manage the data they store. Starting with the Luminous 12.2.z release, the new default (and recommended) backend is BlueStore. 
 Prior to Luminous, the default (and only option) was Filestore.
 
 All Ceph configuration options have a unique name consisting of words formed with lower-case characters and connected with underscore (_) characters.
 
Each Ceph daemon, process, and library will pull its configuration from several sources, listed below. Sources later in the list will override those earlier in 
the list when both are present.
the compiled-in default value
the monitor cluster’s centralized configuration database
a configuration file stored on the local host
environment variables
command line arguments
runtime overrides set by an administrator

Ceph options that are stored in the monitor configuration database or in local configuration files are grouped into sections to indicate which daemons or clients 
they apply to. global,mon,mgr,osd,mds,client are the sections.

 You may deploy Ceph with a single monitor, but if the instance fails, the lack of other monitors may interrupt data service availability.
 
 It is possible to run a Ceph Storage Cluster with two networks: a public (client, front-side) network and a cluster (private, replication, back-side) network. 
 However, this approach complicates network configuration (both hardware and software) and does not usually have a significant impact on overall performance. 
 For this reason, we recommend that for resilience and capacity dual-NIC systems either active/active bond these interfaces or implemebnt a layer 3 multipath 
 strategy with eg. FRR
 
 If you declare a cluster network, OSDs will route heartbeat, object replication and recovery traffic over the cluster network. This may improve performance 
 compared to using a single network.We prefer that the cluster network is NOT reachable from the public network or the Internet for added security.
 
 The messenger v2 protocol, or msgr2, is the second major revision on Ceph’s on-wire protocol.
 # ceph mon dump
0: [v2:10.0.0.10:3300/0,v1:10.0.0.10:6789/0] mon.foo
1: [v2:10.0.0.11:3300/0,v1:10.0.0.11:6789/0] mon.bar
2: [v2:10.0.0.12:3300/0,v1:10.0.0.12:6789/0] mon.baz
The bracketed list or vector of addresses means that the same daemon can be reached on multiple ports (and protocols). Any client or other daemon connecting to 
that daemon will use the v2 protocol (listed first) if possible; otherwise it will back to the legacy v1 protocol. Legacy clients will only see the v1 addresses 
and will continue to connect as they did before, with the v1 protocol.

The cephx protocol is enabled by default. Cryptographic authentication has some computational costs, though they should generally be quite low. If the network 
environment connecting your client and server hosts is very safe and you cannot afford authentication, you can turn it off. This is not generally recommended.

Ceph Monitors maintain a “master copy” of the Cluster Map, which means a Ceph Client can determine the location of all Ceph Monitors, Ceph OSD Daemons, and 
Ceph Metadata Servers just by connecting to one Ceph Monitor and retrieving a current cluster map.
With a current copy of the cluster map and the CRUSH algorithm, a Ceph Client can compute the location for any object.
 
The primary role of the Ceph Monitor is to maintain a master copy of the cluster map. Ceph Monitors also provide authentication and logging services.
When there is a significant change in the state of the cluster–e.g., a Ceph OSD Daemon goes down, a placement group falls into a degraded state, etc.–the 
cluster map gets updated to reflect the current state of the cluster.Additionally, the Ceph Monitor also maintains a history of the prior states of the cluster. 
The monitor map, OSD map, placement group map and metadata server map each maintain a history of their map versions. We call each version an “epoch.”

Ceph Clients and other Ceph daemons use the Ceph configuration file to discover monitors, monitors discover each other using the monitor map (monmap), not the 
Ceph configuration file.

Each Ceph Storage Cluster has a unique identifier (fsid). If specified, it usually appears under the [global] section of the configuration file. Deployment tools 
usually generate the fsid and store it in the monitor map, so the value may not appear in a configuration file. The fsid makes it possible to run daemons for 
multiple clusters on the same hardware.

When a Ceph Storage Cluster gets close to its maximum capacity (see``mon_osd_full ratio``), Ceph prevents you from writing to or reading from OSDs as a safety 
easure to prevent data loss. Therefore, letting a production Ceph Storage Cluster approach its full ratio is not a good practice, because it sacrifices 
high availability

# read https://docs.ceph.com/en/latest/rados/configuration/mon-config-ref/?#storage-capacity for capacity planning
if some OSDs are nearfull, but others have plenty of capacity, you may have an inaccurate CRUSH weight set for the nearfull OSDs.

Ceph monitors know about the cluster by requiring reports from each OSD, and by receiving reports from OSDs about the status of their neighboring OSDs.

When you run a production cluster with multiple monitors (recommended), each monitor checks to see if a neighboring monitor has a more recent version of 
the cluster map.Periodically, one monitor in the cluster may fall behind the other monitors to the point where it must leave the quorum, synchronize to retrieve 
the most current information about the cluster, and then rejoin the quorum.

Ceph daemons pass critical messages to each other, which must be processed before daemons reach a timeout threshold. If the clocks in Ceph monitors are not 
synchronized, it can lead to a number of anomalies. For example:
Daemons ignoring received messages (e.g., timestamps outdated)
Timeouts triggered too soon/late when a message wasn’t received in time.

LOOKING UP MONITORS THROUGH DNS. This way daemons and clients do not require a mon host configuration directive in their ceph.conf configuration file.This way 
daemons and clients do not require a mon host configuration directive in their ceph.conf configuration file.

Ceph ensures data integrity by scrubbing placement groups. Ceph scrubbing is analogous to fsck on the object storage layer. For each placement group, 
Ceph generates a catalog of all objects and compares each primary object and its replicas to ensure that no objects are missing or mismatched. 
Light scrubbing (daily) checks the object size and attributes. Deep scrubbing (weekly) reads the data and uses checksums to ensure data integrity.

Scrubbing is important for maintaining data integrity, but it can reduce performance. 

ceph use mclock for QOS.

When you add or remove Ceph OSD Daemons to a cluster, CRUSH will rebalance the cluster by moving placement groups to or from Ceph OSDs to restore balanced 
utilization. The process of migrating placement groups and the objects they contain can reduce the cluster’s operational performance considerably. 
To maintain operational performance, Ceph performs this migration with ‘backfilling’, which allows Ceph to set backfill operations to a lower priority than 
requests to read or write data.

If a Ceph OSD Daemon crashes and comes back online, usually it will be out of sync with other Ceph OSD Daemons containing more recent versions of objects in the 
placement groups. When this happens, the Ceph OSD Daemon goes into recovery mode and seeks to get the latest copy of the data and bring its map back up to date. 
Depending upon how long the Ceph OSD Daemon was down, the OSD’s objects and placement groups may be significantly out of date. Also, if a failure domain went 
down (e.g., a rack), more than one Ceph OSD Daemon may come back online at the same time. This can make the recovery process time consuming and resource intensive.

Filestore OSDs use a journal for two reasons: speed and consistency. Note that since Luminous, the BlueStore OSD back end has been preferred and default. 
This information is provided for pre-existing OSDs and for rare situations where Filestore is preferred for new deployments.

When you create pools and set the number of placement groups (PGs) for each, Ceph uses default values when you don’t specifically override the defaults. 
We recommend overriding some of the defaults. Specifically, we recommend setting a pool’s replica size and overriding the default number of placement groups.

MON_DOWN: One or more monitor daemons is currently down. The cluster requires a majority (more than 1/2) of the monitors in order to function. When one or more monitors 
are down, clients may have a harder time forming their initial connection to the cluster as they may need to try more addresses before they reach an 
operating monitor.

The cluster should normally have at least one running manager (ceph-mgr) daemon. If no manager daemon is running, 
the cluster’s ability to monitor itself will be compromised, and parts of the management API will become unavailable (for example, the dashboard will not work, 
and most CLI commands that report metrics or runtime state will block). However, the cluster will still be able to perform all IO operations and recover from 
failures.
The down manager daemon should generally be restarted as soon as possible to ensure that the cluster can be monitored (e.g., so that the ceph -s information is 
up to date, and/or metrics can be scraped by Prometheus).

The utilization thresholds for nearfull, backfillfull, full, and/or failsafe_full are not ascending. In particular, we expect 
nearfull < backfillfull, backfillfull < full, and full < failsafe_full.

POOL_FULL: One or more pools has reached its quota and is no longer allowing writes.

# PG_AVAILABILITY
Data availability is reduced, meaning that the cluster is unable to service potential read or write requests for some data in the cluster. Specifically, 
one or more PGs is in a state that does not allow IO requests to be serviced. Problematic PG states include peering, stale, incomplete, and the lack of active 
(if those conditions do not clear quickly).

# PG_DEGRADED
Data redundancy is reduced for some data, meaning the cluster does not have the desired number of replicas for all data (for replicated pools) or 
erasure code fragments (for erasure coded pools).

# OSD_BACKFILLFULL
One or more OSDs has exceeded the backfillfull threshold, which will prevent data from being allowed to rebalance to this device. This is an early warning that 
rebalancing may not be able to complete and that the cluster is approaching full.

The number of PGs in use in the cluster is above the configurable threshold of mon_max_pg_per_osd PGs per OSD. If this threshold is exceed the cluster 
wll not allow new pools to be created, pool pg_num to be increased, or pool replication to be increased (any of which would lead to more PGs in the cluster). 
A large number of PGs can lead to higher memory utilization for OSD daemons, slower peering after cluster state changes 
(like OSD restarts, additions, or removals), and higher load on the Manager and Monitor daemons.
The simplest way to mitigate the problem is to increase the number of OSDs in the cluster by adding more hardware. Note that the OSD count used for the 
purposes of this health check is the number of “in” OSDs, so marking “out” OSDs “in” (if there are any) can also help:
ceph osd in <osd id(s)>

# OBJECT_MISPLACED
One or more objects in the cluster is not stored on the node the cluster would like it to be stored on. This is an indication that data migration due to some 
recent cluster change has not yet completed.
Misplaced data is not a dangerous condition in and of itself; data consistency is never at risk, and old copies of objects are never removed until the desired 
number of new copies (in the desired locations) are present.

High availability and high reliability require a fault-tolerant approach to managing hardware and software issues. Ceph has no single point-of-failure, and can 
service requests for data in a “degraded” mode. Ceph’s data placement introduces a layer of indirection to ensure that data doesn’t bind directly to particular 
OSD addresses. This means that tracking down system faults requires finding the placement group and the underlying OSDs at root of the problem.

# Tip A fault in one part of the cluster may prevent you from accessing a particular object, but that doesn’t mean that you cannot access other objects. When you 
run into a fault, don’t panic. Just follow the steps for monitoring your OSDs and placement groups. Then, begin troubleshooting.

If an OSD is down and in, there is a problem and the cluster will not be in a healthy state.

If the Up Set and Acting Set do not match, this may be an indicator that the cluster rebalancing itself or of a potential problem with the cluster.

Each PG has an attribute called Acting Set, comprising the current primary OSD and presently active replicas. This set of OSDs is responsible for actively 
serving I/O at the given moment in time. When the cluster topology changes, a PG may be updated to span a different set of OSDs. This new set of OSDs might or 
might not overlap with the previous set and is known as the Up Set of each PG. The OSDs in the Up Set are responsible for transferring data over from the current 
Acting Set's OSDs because they are now responsible for the PG that was adjusted. Once the data is current on all OSDs in the Up Set, the Acting Set will be the 
same as the Up Set and the PG should be in an active state.

Before you can write data to a placement group, it must be in an active state, and it should be in a clean state. For Ceph to determine the current state of 
a placement group, the primary OSD of the placement group (i.e., the first OSD in the acting set), peers with the secondary and tertiary OSDs to establish 
agreement on the current state of the placement group (assuming a pool with 3 replicas of the PG).

 An important aspect of monitoring placement groups is to ensure that when the cluster is up and running that all placement groups are active, and preferably in 
 the clean state.
 
 Placement group IDs consist of the pool number (not pool name) followed by a period (.) and the placement group ID–a hexadecimal number. You can view 
 pool numbers and their names from the output of ceph osd lspools. For example, the first pool created corresponds to pool number 1. A fully qualified 
 placement group ID has the following form:
{pool-num}.{pg-id}

To store object data in the Ceph Object Store, a Ceph client must:
1. Set an object name
2. Specify a pool

# The following subsections describe the common pg states
CREATING: When you create a pool, it will create the number of placement groups you specified. Ceph will echo creating when it is creating one or more placement 
groups. Once they are created, the OSDs that are part of a placement group’s Acting Set will peer. Once peering is complete, the placement group status should be 
active+clean, which means a Ceph client can begin writing to the placement group.

PEERING: When Ceph is Peering a placement group, Ceph is bringing the OSDs that store the replicas of the placement group into agreement about the state of the 
objects and metadata in the placement group. When Ceph completes peering, this means that the OSDs that store the placement group agree about the current state 
of the placement group. However, completion of the peering process does NOT mean that each replica has the latest contents.

ACTIVE: Once Ceph completes the peering process, a placement group may become active. The active state means that the data in the placement group is generally 
available in the primary placement group and the replicas for read and write operations.

CLEAN: When a placement group is in the clean state, the primary OSD and the replica OSDs have successfully peered and there are no stray replicas for the 
placement group. Ceph replicated all objects in the placement group the correct number of times

DEGRADED: When a client writes an object to the primary OSD, the primary OSD is responsible for writing the replicas to the replica OSDs. After the primary OSD 
writes the object to storage, the placement group will remain in a degraded state until the primary OSD has received an acknowledgement from the replica OSDs 
that Ceph created the replica objects successfully.

RECOVERING: When an OSD goes down, its contents may fall behind the current state of other replicas in the placement groups. When the OSD is back up, the contents of 
the placement groups must be updated to reflect the current state. During that time period, the OSD may reflect a recovering state.

BACK FILLING: When a new OSD joins the cluster, CRUSH will reassign placement groups from OSDs in the cluster to the newly added OSD. Forcing the new OSD to 
accept the reassigned placement groups immediately can put excessive load on the new OSD. Back filling the OSD with the placement groups allows this process to 
begin in the background. Once backfilling is complete, the new OSD will begin serving requests when it is ready.

REMAPPED: When the Acting Set that services a placement group changes, the data migrates from the old acting set to the new acting set. It may take some time for 
a new primary OSD to service requests. So it may ask the old primary to continue to service requests until the placement group migration is complete. 
Once data migration completes, the mapping uses the primary OSD of the new acting set.

STALE: If the Primary OSD of a placement group’s acting set fails to report to the monitor or if other OSDs have reported the primary OSD down, the monitors will 
mark the placement group stale. After your cluster has been running for awhile, seeing placement groups in the stale state indicates that the primary OSD for 
those placement groups is down or not reporting placement group statistics to the monitor.

# As previously noted, a placement group is not necessarily problematic just because its state is not active+clean. Generally, Ceph’s ability to self repair may 
not be working when placement groups get stuck. The stuck states include:
Unclean: Placement groups contain objects that are not replicated the desired number of times. They should be recovering.
Inactive: Placement groups cannot process reads or writes because they are waiting for an OSD with the most up-to-date data to come back up.
Stale: Placement groups are in an unknown state, because the OSDs that host them have not reported to the monitor cluster in a while (configured by mon osd 
report timeout).

Users are either individuals or system actors such as applications, which use Ceph clients to interact with the Ceph Storage Cluster daemons.

Ceph has the notion of a type of user. For the purposes of user management, the type will always be client. Ceph identifies users in period (.) delimited form 
consisting of the user type and the user ID: for example, TYPE.ID, client.admin, or client.user1. The reason for user typing is that Ceph Monitors, OSDs, and 
Metadata Servers also use the Cephx protocol, but they are not clients. Distinguishing the user type helps to distinguish between client users and other 
users–streamlining access control, user monitoring and traceability.

A pool is a logical partition where users store data. In Ceph deployments, it is common to create a pool as a logical partition for similar types of data. For 
example, when deploying Ceph as a backend for OpenStack, a typical deployment would have pools for volumes, images, backups and virtual machines, and users such 
as client.glance, client.cinder, etc.

Objects within a pool can be associated to a namespace–a logical group of objects within the pool. A user’s access to a pool can be associated with a namespace 
such that reads and writes by the user take place only within the namespace. Objects written to a namespace within the pool can only be accessed by users who 
have access to the namespace.

When you access Ceph via a Ceph client, the Ceph client will look for a local keyring. Ceph presets the keyring setting with the following four keyring names by 
default so you don’t have to set them in your Ceph configuration file unless you want to override the defaults (not recommended):
/etc/ceph/$cluster.$name.keyring
/etc/ceph/$cluster.keyring
/etc/ceph/keyring
/etc/ceph/keyring.bin
The $cluster metavariable is your Ceph cluster name as defined by the name of the Ceph configuration file (i.e., ceph.conf means the cluster name is ceph; thus, 
ceph.keyring). The $name metavariable is the user type and user ID (e.g., client.admin; thus, ceph.client.admin.keyring).

Ceph stores and updates the checksums of objects stored in the cluster. When a scrub is performed on a placement group, the OSD attempts to choose an 
authoritative copy from among its replicas. Among all of the possible cases, only one case is consistent. After a deep scrub, Ceph calculates the checksum of an 
object read from the disk and compares it to the checksum previously recorded. If the current checksum and the previously recorded checksums do not match, that 
is an inconsistency. In the case of replicated pools, any mismatch between the checksum of any replica of an object and the checksum of the authoritative copy 
means that there is an inconsistency.

Erasure coded pools require more resources than replicated pools and lack some functionalities such as omap. but it consume less storage than replicated pool.

Ceph OSD Daemons perform optimally when all storage drives in the rule are of the same size, speed (both RPMs and throughput) and type

Backfill is a special case of recovery.

A placement group (PG) aggregates objects within a pool because tracking object placement and object metadata on a per-object basis is computationally 
expensive–i.e., a system with millions of objects cannot realistically track placement on a per-object basis.

It is advisable to run an odd number of monitors. An odd number of monitors is more resilient than an even number. For instance, with a two monitor deployment, 
no failures can be tolerated and still maintain a quorum; with three monitors, one failure can be tolerated; in a four monitor deployment, one failure can be 
tolerated; with five monitors, two failures can be tolerated. This avoids the dreaded split brain phenomenon, and is why an odd number is best. In short, Ceph 
needs a majority of monitors to be active (and able to communicate with each other), but that majority can be achieved using a single monitor, or 
2 out of 2 monitors, 2 out of 3, 3 out of 4, etc.

Since monitors are lightweight, it is possible to run them on the same host as OSDs; however, we recommend running them on separate hosts, because fsync issues 
with the kernel may impair performance. Dedicated monitor nodes also minimize disruption since monitor and OSD daemons are not inactive at the same time when a 
node crashes or is taken down for maintenance.

Dedicated monitor nodes also make for cleaner maintenance by avoiding both OSDs and a mon going down if a node is rebooted, taken down, or crashes.

A monitor always refers to the local copy of the monmap when discovering other monitors in the cluster. Using the monmap instead of ceph.conf avoids errors that 
could break the cluster (e.g., typos in ceph.conf when specifying a monitor address or port).
the monmap provides monitors with a strict guarantee that their consensus is valid.

Strict consistency also applies to updates to the monmap. As with any other updates on the monitor, changes to the monmap always run through a distributed 
consensus algorithm called Paxos. The monitors must agree on each update to the monmap, such as adding or removing a monitor, to ensure that each monitor in the 
quorum has the same version of the monmap. Updates to the monmap are incremental so that monitors have the latest agreed upon version, and a set of previous 
versions, allowing a monitor that has an older version of the monmap to catch up with the current state of the cluster.

If monitors discovered each other through the Ceph configuration file instead of through the monmap, it would introduce additional risks because the 
Ceph configuration files are not updated and distributed automatically.

Each OSD can run either BlueStore or FileStore, and a single Ceph cluster can contain a mix of both. Users who have previously deployed FileStore are likely to 
want to transition to BlueStore in order to take advantage of the improved performance and robustness.

When common networking technloogies were 100Mb/s and 1Gb/s, this separation of public and private was often critical. With today’s 10Gb/s, 40Gb/s, and 25/50/100Gb/s networks, 
the above capacity concerns are often diminished or even obviated. For example, if your OSD nodes have two network ports, dedicating one to the public and the 
other to the private network means no path redundancy. This degrades your ability to weather network maintenance and failures without significant cluster or 
client impact. Consider instead using both links for just a public network: with bonding (LACP) or equal-cost routing (e.g. FRR) you reap the benefits of 
increased throughput headroom, fault tolerance, and reduced OSD flapping.

Logging is resource intensive. If you are encountering a problem in a specific area of your cluster, enable logging for that area of the cluster. For example, 
if your OSDs are running fine, but your metadata servers are not, you should start by enabling debug logging for the specific metadata server instance(s) giving 
you trouble. Enable logging for each subsystem as needed.

Verbose logging can generate over 1GB of data per hour. If your OS disk reaches its capacity, the node will stop working.

### Ceph block device https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/
Ceph block devices are thin-provisioned, resizable, and store data striped over multiple OSDs. Ceph block devices leverage RADOS capabilities including 
snapshotting, replication and strong consistency. Ceph block storage clients communicate with Ceph clusters through kernel modules or the librbd library.
Ceph’s block devices deliver high performance with vast scalability to kernel modules, or to KVMs such as QEMU, and cloud-based computing systems like OpenStack 
and CloudStack that rely on libvirt and QEMU to integrate with Ceph block devices.

Because RBD does not know about any filesystem within an image (volume), snapshots are not crash-consistent unless they are coordinated within the mounting 
(attaching) operating system. We therefore recommend that you pause or stop I/O before taking a snapshot. If the volume contains a filesystem, it must be in an 
internally consistent state before taking a snapshot. Snapshots taken at inconsistent points may need a fsck pass before subsequent mounting. To stop I/O you can 
use fsfreeze command. See fsfreeze(8) man page for more details. For virtual machines, qemu-guest-agent can be used to automatically freeze file systems when 
creating a snapshot.

Ceph supports the ability to create many copy-on-write (COW) clones of a block device snapshot. Snapshot layering enables Ceph block device clients to create 
images very quickly. For example, you might create a block device image with a Linux VM written to it; then, snapshot the image, protect the snapshot, and create 
as many copy-on-write clones as you like. A snapshot is read-only, so cloning a snapshot simplifies semantics–making it possible to create clones rapidly.

Each cloned image (child) stores a reference to its parent image, which enables the cloned image to open the parent snapshot and read it.

Ceph block device layering is a simple process. You must have an image. You must create a snapshot of the image. You must protect the snapshot. Once you have 
performed these steps, you can begin cloning the snapshot.

The cloned image has a reference to the parent snapshot, and includes the pool ID, image ID and snapshot ID. The inclusion of the pool ID means that you may 
clone snapshots from one pool to images in another pool.

Exclusive locks are a mechanism designed to prevent multiple processes from accessing the same Rados Block Device (RBD) in an uncoordinated fashion. 
Exclusive locks are heavily used in virtualization (where they prevent VMs from clobbering each others’ writes), and also in RBD mirroring 
(where they are a prerequisite for journaling).

Note that it is perfectly possible for two or more concurrently running processes to merely open the image, and also to read from it. The client acquires the 
exclusive lock only when attempting to write to the image.

RBD images can be asynchronously mirrored between two Ceph clusters.

RBD images can be live-migrated between different pools within the same cluster; between different image formats and layouts; or from external data sources. 
When started, the source will be deep-copied to the destination image, pulling all snapshot history while preserving the sparse allocation of data where possible.

Cloned RBD images usually modify only a small fraction of the parent image. For example, in a VDI use-case, VMs are cloned from the same base image and initially 
differ only by hostname and IP address. During booting, all of these VMs read portions of the same parent image data. If we have a local cache of the parent image,
this speeds up reads on the caching host. We also achieve reduction of client-to-cluster network traffic. RBD cache must be explicitly enabled in ceph.conf. 
The ceph-immutable-object-cache daemon is responsible for caching the parent content on the local disk, and future reads on that data will be serviced from the 
local cache.

The persistent write-back cache provides a persistent, fault-tolerant write-back cache for librbd-based RBD clients.

Starting with the Pacific release, image-level encryption can be handled internally by RBD clients. This means you can set a secret key that will be used to 
encrypt a specific RBD image.




