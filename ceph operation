if one of the osd reach full ratio you can increase the full ratio ( before decreasing it is good to delete the unneccessary objects to queue them) 
and then delete the unnecessary objects then set back the full ratio. # you can use below link
https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html-single/troubleshooting_guide/index#deleting-data-from-a-full-storage-cluster_diag

### Architecture https://docs.ceph.com/en/latest/architecture/
# check ceph cluster map
ceph mon dump
ceph osd dump
ceph pg dump
ceph fs dump # This is used for file storage
osd getcrushmap -o {filename}; crushtool -d {comp-crushmap-filename} -o {decomp-crushmap-filename};

### Ceph Storage Cluster https://docs.ceph.com/en/latest/rados/
ceph config show <who> # show the config of <who>
ceph config show-with-defaults <who> # show the config options with defaults
# get help for config option
ceph config help <option>
# temporarily set an option using the tell or daemon interfaces on the Ceph CLI. These override values are ephemeral in that they only affect the running process 
and are discarded/lost if the daemon or process restarts.
ceph tell <name> config set <option> <value>
ceph daemon <name> config set <option> <value>
Note that in the ceph config show command output these temporary values will be shown with a source of override.

### Ceph Operation https://docs.ceph.com/en/latest/rados/operations/
#start all daemon
sudo systemctl start ceph.target
#stop all daemon
sudo systemctl stop ceph\*.service ceph\*.target

#To start all daemons of a particular type on a Ceph Node, execute one of the following:
sudo systemctl start ceph-osd.target
sudo systemctl start ceph-mon.target
sudo systemctl start ceph-mds.target

#To stop all daemons of a particular type on a Ceph Node, execute one of the following:
sudo systemctl stop ceph-mon\*.service ceph-mon.target
sudo systemctl stop ceph-osd\*.service ceph-osd.target
sudo systemctl stop ceph-mds\*.service ceph-mds.target

To start/stop a specific daemon instance on a Ceph Node, execute one of the following:
sudo systemctl start/stop ceph-osd@{id}
sudo systemctl start/stop ceph-mon@{hostname}
sudo systemctl start/stop ceph-mds@{hostname}

# Utilization by pool can be checked with
ceph df
ceph osd dump | grep full_ratio

ceph osd set <flag>
ceph osd unset <flag>
<flag>: full pause noup nodown noin noout nobackfill, norecover norebalance noscrub nodeep_scrub  notieragent

ceph osd set-group <flags> <who>
ceph osd unset-group <flags> <who>
<flag>: noup nodown noin noout 
<who>: osd.0,osd.1,...

# Detailed information about which PGs are affected is available from:
ceph health detail
# the state of specific problematic PGs can be queried with:
ceph tell <pgid> query

ceph osd in <osd id(s)>

# The request queue for the daemon in question can be queried with the following command, executed from the daemon’s host:
ceph daemon osd.<id> ops
# A summary of the slowest recent requests can be seen with:
ceph daemon osd.<id> dump_historic_ops
# The location of an OSD can be found with:
ceph osd find osd.<id>

# You can manually initiate a scrub of a clean PG with:
ceph pg scrub <pgid>
ceph pg deep-scrub <pgid>

# New crashes can be listed with:
ceph crash ls-new
# Information about a specific crash can be examined with:
ceph crash info <crash-id>
# This warning can be silenced by “archiving” the crash (perhaps after being examined by an administrator) so that it does not generate this warning:
ceph crash archive <crash-id>
# Similarly, all new crashes can be archived with:
ceph crash archive-all

#The telemetry module sends anonymous data about the cluster back to the Ceph developers to help understand how Ceph is used and what problems users may be 
experiencing.
ceph telemetry show

ceph
ceph> health
ceph> status
ceph> quorum_status
ceph> mon stat

# In addition to local logging by each daemon, Ceph clusters maintain a cluster log that records high level events about the whole system. This is logged to disk 
on monitor servers (as /var/log/ceph/ceph.log by default), but can also be monitored via the command line.
ceph -w
In addition to using ceph -w to print log lines as they are emitted, use ceph log last [n] to see the most recent n lines from the cluster log.
ceph log last [n]

# The following command will show all gathered network performance data by specifying a threshold of 0 and sending to the mgr.
ceph daemon /var/run/ceph/ceph-mgr.x.asok dump_osd_network 0

# Health checks can be muted so that they do not affect the overall reported status of the cluster. 
ceph health mute <code>
A mute can be explicitly removed with:
ceph health unmute <code>

# To check a cluster’s data usage and data distribution among pools, you can use the df option. It is similar to Linux df
ceph df 
ceph df detail

# You can check OSDs to ensure they are up and in by executing the following command:
ceph osd stat
ceph osd dump
ceph osd tree

# To see display the monitor map, execute the following:
ceph mon stat
ceph mon dump
ceph quorum_status

# The Ceph admin socket allows you to query a daemon via a socket interface. By default, Ceph sockets reside under /var/run/ceph. To access a daemon via the 
admin socket, login to the host running the daemon and use the following command:
ceph daemon {daemon-name}
ceph daemon {path-to-socket-file}

#To view the available admin socket commands, execute the following command:
ceph daemon {daemon-name} help

# To retrieve a list of placement groups, execute:
ceph pg dump

# to view which OSDs are within the Acting Set or the Up Set for a given placement group, execute:
ceph pg map {pg-num}

ceph pg stat
ceph ods lspools

To query a particular placement group, execute the following:
ceph pg {poolnum}.{pg-id} query

# To identify stuck placement groups, execute the following:
ceph pg dump_stuck [unclean|inactive|stale|undersized|degraded]

# To find the object location, all you need is the object name and the pool name
ceph osd map {poolname} {object-name} [namespace]

If you do not specify a user name, Ceph will use client.admin as the default user name. If you do not specify a keyring, Ceph will look for a keyring via the 
keyring setting in the Ceph configuration
ceph health = ceph -n client.admin --keyring=/etc/ceph/ceph.client.admin.keyring health

# To list the users in your cluster, execute the following:
ceph auth ls

# To retrieve a specific user, key and capabilities, execute the following:
ceph auth get {TYPE.ID}

# COMMANDS FOR DIAGNOSING PLACEMENT-GROUP PROBLEMS
ceph health detail
The following command provides more detail on the status of the placement groups:
ceph pg dump --format=json-pretty
The following command lists inconsistent placement groups
rados list-inconsistent-pg {pool}
The following command lists inconsistent rados objects:
rados list-inconsistent-obj {pgid}
The following command lists inconsistent snapsets in the given placement group:
rados list-inconsistent-snapset {pgid}

To show a pool’s utilization statistics, execute:
rados df
Additionally, to obtain I/O information for a specific pool or all, execute:
ceph osd pool stats [{pool-name}]

To get a value from a pool, execute the following:
ceph osd pool get {pool-name} {key}

You can view each pool, its relative utilization, and any suggested changes to the PG count with this command:
ceph osd pool autoscale-status

# You can query which storage devices are in use with:
ceph device ls
ceph device info <devid>

# Set the override weight (reweight) of {osd-num} to {weight}. Two OSDs with the same weight will receive roughly the same number of I/O requests and store 
approximately the same amount of data. ceph osd reweight sets an override weight on the OSD. This value is in the range 0 to 1, and forces CRUSH to re-place 
(1-weight) of the data that would otherwise live on this drive.
ceph osd reweight {osd-num} {weight}

# link for command ref: https://docs.ceph.com/en/latest/rados/operations/control/#
