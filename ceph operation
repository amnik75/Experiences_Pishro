if one of the osd reach full ratio you can increase the full ratio ( before decreasing it is good to delete the unneccessary objects to queue them) 
and then delete the unnecessary objects then set back the full ratio. # you can use below link
https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html-single/troubleshooting_guide/index#deleting-data-from-a-full-storage-cluster_diag

### Architecture https://docs.ceph.com/en/latest/architecture/
# check ceph cluster map
ceph mon dump
ceph osd dump
ceph pg dump
ceph fs dump # This is used for file storage
osd getcrushmap -o {filename}; crushtool -d {comp-crushmap-filename} -o {decomp-crushmap-filename};

### Ceph Storage Cluster https://docs.ceph.com/en/latest/rados/
ceph config show <who> # show the config of <who>
ceph config show-with-defaults <who> # show the config options with defaults
# get help for config option
ceph config help <option>
# temporarily set an option using the tell or daemon interfaces on the Ceph CLI. These override values are ephemeral in that they only affect the running process 
and are discarded/lost if the daemon or process restarts.
ceph tell <name> config set <option> <value>
ceph daemon <name> config set <option> <value>
Note that in the ceph config show command output these temporary values will be shown with a source of override.

### Ceph Operation https://docs.ceph.com/en/latest/rados/operations/
#start all daemon
sudo systemctl start ceph.target
#stop all daemon
sudo systemctl stop ceph\*.service ceph\*.target

#To start all daemons of a particular type on a Ceph Node, execute one of the following:
sudo systemctl start ceph-osd.target
sudo systemctl start ceph-mon.target
sudo systemctl start ceph-mds.target

#To stop all daemons of a particular type on a Ceph Node, execute one of the following:
sudo systemctl stop ceph-mon\*.service ceph-mon.target
sudo systemctl stop ceph-osd\*.service ceph-osd.target
sudo systemctl stop ceph-mds\*.service ceph-mds.target

To start/stop a specific daemon instance on a Ceph Node, execute one of the following:
sudo systemctl start/stop ceph-osd@{id}
sudo systemctl start/stop ceph-mon@{hostname}
sudo systemctl start/stop ceph-mds@{hostname}

# Utilization by pool can be checked with
ceph df
ceph osd dump | grep full_ratio

ceph osd set <flag>
ceph osd unset <flag>
<flag>: full pause noup nodown noin noout nobackfill, norecover norebalance noscrub nodeep_scrub  notieragent

ceph osd set-group <flags> <who>
ceph osd unset-group <flags> <who>
<flag>: noup nodown noin noout 
<who>: osd.0,osd.1,...

# Detailed information about which PGs are affected is available from:
ceph health detail
# the state of specific problematic PGs can be queried with:
ceph tell <pgid> query

ceph osd in <osd id(s)>

# The request queue for the daemon in question can be queried with the following command, executed from the daemon’s host:
ceph daemon osd.<id> ops
# A summary of the slowest recent requests can be seen with:
ceph daemon osd.<id> dump_historic_ops
# The location of an OSD can be found with:
ceph osd find osd.<id>

# You can manually initiate a scrub of a clean PG with:
ceph pg scrub <pgid>
ceph pg deep-scrub <pgid>

# New crashes can be listed with:
ceph crash ls-new
# Information about a specific crash can be examined with:
ceph crash info <crash-id>
# This warning can be silenced by “archiving” the crash (perhaps after being examined by an administrator) so that it does not generate this warning:
ceph crash archive <crash-id>
# Similarly, all new crashes can be archived with:
ceph crash archive-all

#The telemetry module sends anonymous data about the cluster back to the Ceph developers to help understand how Ceph is used and what problems users may be 
experiencing.
ceph telemetry show

ceph
ceph> health
ceph> status
ceph> quorum_status
ceph> mon stat

# In addition to local logging by each daemon, Ceph clusters maintain a cluster log that records high level events about the whole system. This is logged to disk 
on monitor servers (as /var/log/ceph/ceph.log by default), but can also be monitored via the command line.
ceph -w
In addition to using ceph -w to print log lines as they are emitted, use ceph log last [n] to see the most recent n lines from the cluster log.
ceph log last [n]

# The following command will show all gathered network performance data by specifying a threshold of 0 and sending to the mgr.
ceph daemon /var/run/ceph/ceph-mgr.x.asok dump_osd_network 0

# Health checks can be muted so that they do not affect the overall reported status of the cluster. 
ceph health mute <code>
A mute can be explicitly removed with:
ceph health unmute <code>

# To check a cluster’s data usage and data distribution among pools, you can use the df option. It is similar to Linux df
ceph df 
ceph df detail

# You can check OSDs to ensure they are up and in by executing the following command:
ceph osd stat
ceph osd dump
ceph osd tree

# To see display the monitor map, execute the following:
ceph mon stat
ceph mon dump
ceph quorum_status

# The Ceph admin socket allows you to query a daemon via a socket interface. By default, Ceph sockets reside under /var/run/ceph. To access a daemon via the 
admin socket, login to the host running the daemon and use the following command:
ceph daemon {daemon-name}
ceph daemon {path-to-socket-file}

#To view the available admin socket commands, execute the following command:
ceph daemon {daemon-name} help

