if one of the osd reach full ratio you can increase the full ratio ( before decreasing it is good to delete the unneccessary objects to queue them) 
and then delete the unnecessary objects then set back the full ratio. # you can use below link
https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html-single/troubleshooting_guide/index#deleting-data-from-a-full-storage-cluster_diag

### Architecture https://docs.ceph.com/en/latest/architecture/
# check ceph cluster map
ceph mon dump
ceph osd dump
ceph pg dump
ceph fs dump # This is used for file storage
osd getcrushmap -o {filename}; crushtool -d {comp-crushmap-filename} -o {decomp-crushmap-filename};

### Ceph Storage Cluster https://docs.ceph.com/en/latest/rados/
ceph config show <who> # show the config of <who>
ceph config show-with-defaults <who> # show the config options with defaults
# get help for config option
ceph config help <option>
# temporarily set an option using the tell or daemon interfaces on the Ceph CLI. These override values are ephemeral in that they only affect the running process 
and are discarded/lost if the daemon or process restarts.
ceph tell <name> config set <option> <value>
ceph daemon <name> config set <option> <value>
Note that in the ceph config show command output these temporary values will be shown with a source of override.

### Ceph Operation https://docs.ceph.com/en/latest/rados/operations/
#start all daemon
sudo systemctl start ceph.target
#stop all daemon
sudo systemctl stop ceph\*.service ceph\*.target

#To start all daemons of a particular type on a Ceph Node, execute one of the following:
sudo systemctl start ceph-osd.target
sudo systemctl start ceph-mon.target
sudo systemctl start ceph-mds.target

#To stop all daemons of a particular type on a Ceph Node, execute one of the following:
sudo systemctl stop ceph-mon\*.service ceph-mon.target
sudo systemctl stop ceph-osd\*.service ceph-osd.target
sudo systemctl stop ceph-mds\*.service ceph-mds.target

To start/stop a specific daemon instance on a Ceph Node, execute one of the following:
sudo systemctl start/stop ceph-osd@{id}
sudo systemctl start/stop ceph-mon@{hostname}
sudo systemctl start/stop ceph-mds@{hostname}

# Utilization by pool can be checked with
ceph df
ceph osd dump | grep full_ratio

ceph osd set <flag>
ceph osd unset <flag>
<flag>: full pause noup nodown noin noout nobackfill, norecover norebalance noscrub nodeep_scrub  notieragent

ceph osd set-group <flags> <who>
ceph osd unset-group <flags> <who>
<flag>: noup nodown noin noout 
<who>: osd.0,osd.1,...

# Detailed information about which PGs are affected is available from:
ceph health detail
# the state of specific problematic PGs can be queried with:
ceph tell <pgid> query

ceph osd in <osd id(s)>

# The request queue for the daemon in question can be queried with the following command, executed from the daemon’s host:
ceph daemon osd.<id> ops
# A summary of the slowest recent requests can be seen with:
ceph daemon osd.<id> dump_historic_ops
# The location of an OSD can be found with:
ceph osd find osd.<id>

# You can manually initiate a scrub of a clean PG with:
ceph pg scrub <pgid>
ceph pg deep-scrub <pgid>

# New crashes can be listed with:
ceph crash ls-new
# Information about a specific crash can be examined with:
ceph crash info <crash-id>
# This warning can be silenced by “archiving” the crash (perhaps after being examined by an administrator) so that it does not generate this warning:
ceph crash archive <crash-id>
# Similarly, all new crashes can be archived with:
ceph crash archive-all

#The telemetry module sends anonymous data about the cluster back to the Ceph developers to help understand how Ceph is used and what problems users may be 
experiencing.
ceph telemetry show

ceph
ceph> health
ceph> status
ceph> quorum_status
ceph> mon stat

# In addition to local logging by each daemon, Ceph clusters maintain a cluster log that records high level events about the whole system. This is logged to disk 
on monitor servers (as /var/log/ceph/ceph.log by default), but can also be monitored via the command line.
ceph -w
In addition to using ceph -w to print log lines as they are emitted, use ceph log last [n] to see the most recent n lines from the cluster log.
ceph log last [n]

# The following command will show all gathered network performance data by specifying a threshold of 0 and sending to the mgr.
ceph daemon /var/run/ceph/ceph-mgr.x.asok dump_osd_network 0

# Health checks can be muted so that they do not affect the overall reported status of the cluster. 
ceph health mute <code>
A mute can be explicitly removed with:
ceph health unmute <code>

# To check a cluster’s data usage and data distribution among pools, you can use the df option. It is similar to Linux df
ceph df 
ceph df detail

# You can check OSDs to ensure they are up and in by executing the following command:
ceph osd stat
ceph osd dump
ceph osd tree

# To see display the monitor map, execute the following:
ceph mon stat
ceph mon dump
ceph quorum_status

# The Ceph admin socket allows you to query a daemon via a socket interface. By default, Ceph sockets reside under /var/run/ceph. To access a daemon via the 
admin socket, login to the host running the daemon and use the following command:
ceph daemon {daemon-name}
ceph daemon {path-to-socket-file}

#To view the available admin socket commands, execute the following command:
ceph daemon {daemon-name} help

# To retrieve a list of placement groups, execute:
ceph pg dump

# to view which OSDs are within the Acting Set or the Up Set for a given placement group, execute:
ceph pg map {pg-num}

ceph pg stat
ceph ods lspools

To query a particular placement group, execute the following:
ceph pg {poolnum}.{pg-id} query

# To identify stuck placement groups, execute the following:
ceph pg dump_stuck [unclean|inactive|stale|undersized|degraded]

# To find the object location, all you need is the object name and the pool name
ceph osd map {poolname} {object-name} [namespace]

If you do not specify a user name, Ceph will use client.admin as the default user name. If you do not specify a keyring, Ceph will look for a keyring via the 
keyring setting in the Ceph configuration
ceph health = ceph -n client.admin --keyring=/etc/ceph/ceph.client.admin.keyring health

# To list the users in your cluster, execute the following:
ceph auth ls

# To retrieve a specific user, key and capabilities, execute the following:
ceph auth get {TYPE.ID}

# COMMANDS FOR DIAGNOSING PLACEMENT-GROUP PROBLEMS
ceph health detail
The following command provides more detail on the status of the placement groups:
ceph pg dump --format=json-pretty
The following command lists inconsistent placement groups
rados list-inconsistent-pg {pool}
The following command lists inconsistent rados objects:
rados list-inconsistent-obj {pgid}
The following command lists inconsistent snapsets in the given placement group:
rados list-inconsistent-snapset {pgid}

To show a pool’s utilization statistics, execute:
rados df
Additionally, to obtain I/O information for a specific pool or all, execute:
ceph osd pool stats [{pool-name}]

To get a value from a pool, execute the following:
ceph osd pool get {pool-name} {key}

You can view each pool, its relative utilization, and any suggested changes to the PG count with this command:
ceph osd pool autoscale-status

# You can query which storage devices are in use with:
ceph device ls
ceph device info <devid>

# Set the override weight (reweight) of {osd-num} to {weight}. Two OSDs with the same weight will receive roughly the same number of I/O requests and store 
approximately the same amount of data. ceph osd reweight sets an override weight on the OSD. This value is in the range 0 to 1, and forces CRUSH to re-place 
(1-weight) of the data that would otherwise live on this drive.
ceph osd reweight {osd-num} {weight}

# link for command ref: https://docs.ceph.com/en/latest/rados/operations/control/#

# to see report of cluster use this command:
ceph report

If the answer is yes then your cluster is up and running. One thing you can take for granted is that the monitors will only answer to a status request if there 
is a formed quorum. Also check that at least one mgr daemon is reported as running, ideally all of them.

If ceph -s hangs without obtaining a reply from the cluster or showing fault messages, then it is likely that your monitors are either down completely or just a 
raction are up – a fraction insufficient to form a majority quorum. This check will connect to an arbitrary mon;

# You can contact each monitor individually asking them for their status, regardless of a quorum being formed. This can be achieved using 
ceph tell mon.ID mon_status, ID being the monitor’s identifier. You should perform this for each monitor in the cluster.
ceph tell mon.ID mon_status

#  Check your networks to ensure they are running properly, because networks may have a significant impact on OSD operation and performance. Look for 
dropped packets on the host side and CRC errors on the switch side.

Periodically, you may need to perform maintenance on a subset of your cluster, or resolve a problem that affects a failure domain (e.g., a rack). If you do not 
want CRUSH to automatically rebalance the cluster as you stop OSDs for maintenance, set the cluster to noout first:
ceph osd set noout
# On Luminous or newer releases it is safer to set the flag only on affected OSDs. You can do this individually
ceph osd add-noout osd.0
ceph osd rm-noout  osd.0
Once the flag is set you can stop the OSDs and any other colocated Ceph services within the failure domain that requires maintenance work.
systemctl stop ceph\*.service ceph\*.target
Once you have completed your maintenance, restart the OSDs and any other daemons. If you rebooted the host as part of the maintenance, these should come back on 
their own without intervention.
ceph osd unset noout
ceph osd unset-group noout prod-ceph-data1701

if you run ceph daemon osd.<id> dump_historic_ops or ceph daemon osd.<id> dump_ops_in_flight, you will see a set of operations and a list of events each 
operation went through.

ceph pg {pgid} query

# If all possible locations have been queried and objects are still lost, you may have to give up on the lost objects. This, again, is possible given unusual 
combinations of failures that allow the cluster to learn about writes that were performed before the writes themselves are recovered. To mark the 
“unfound” objects as “lost”:
ceph pg {pgid} mark_unfound_lost revert|delete

# Replace {daemon-type} with one of osd, mon or mds. You may apply the runtime setting to all daemons of a particular type with *, or specify a specific daemon’s 
ID. For example, to increase debug logging for a ceph-osd daemon named osd.0, execute the following:
ceph tell osd.0 config set debug_osd 0/5

# The ceph tell command goes through the monitors. If you cannot bind to the monitor, you can still make the change by logging into the host of the daemon whose 
configuration you’d like to change using ceph daemon. For example:
sudo ceph daemon osd.0 config set debug_osd 0/5

### Ceph block device https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/

# To list block devices in a particular pool, execute the following:
rbd ls {poolname}

# To retrieve information from an image within a pool, execute the following, but replace {image-name} with the name of the image and replace {pool-name} with the 
name of the pool:
rbd info {pool-name}/{image-name}

# Ceph Block Device images are thin provisioned. They don’t actually use any physical storage until you begin saving data to them. However, they do have a maximum 
capacity that you set with the --size option. If you want to increase (or decrease) the maximum size of a Ceph Block Device image, execute the following:
rbd resize --size 2048 foo (to increase)
rbd resize --size 2048 foo --allow-shrink (to decrease)

# To remove a block device from a pool, execute the following, but replace {image-name} with the name of the image to remove and replace {pool-name} with the 
name of the pool:
rbd rm {pool-name}/{image-name}

To list deferred delete block devices in a particular pool, execute the following, but replace {poolname} with the name of the pool:
rbd trash ls {poolname}

To defer delete a block device from a pool, execute the following, but replace {image-name} with the name of the image to move and replace {pool-name} with the 
name of the pool:
rbd trash mv {pool-name}/{image-name}

To restore a deferred delete block device in a particular pool, execute the following, but replace {image-id} with the id of the image and replace {pool-name} 
with the name of the pool:
rbd trash restore {pool-name}/{image-id}









